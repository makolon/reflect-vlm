# ReflectVLM

Official implementation of ["Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation"](https://arxiv.org/abs/2502.16707).

[Paper](https://arxiv.org/abs/2502.16707) | [Website](https://reflect-vlm.github.io/) | [Video](https://www.youtube.com/watch?v=sZ81C8VXJa0) | [Hugging Face](https://huggingface.co/collections/yunhaif/reflectvlm-67b95e4316ab2d5f71ad4b25)

<center>
<img src="assets/images/teaser.png" width=60%>
</center>


## Contents
- [Installation](#installation)
- [Simulation environment](#simulation-environment)
- [Policy evaluation](#policy-evaluation)
- [Policy training](#policy-training)
- [Citation](#citation)
- [License & Acknowledgements](#license--acknowledgements)


## Docker (with uv) Usage

You can use Docker with [uv](https://github.com/astral-sh/uv) for fast Python dependency management. This is the recommended way to get started quickly and reproducibly.

### Build the Docker image
```bash
docker compose build
```

### Start a container
```bash
docker compose run --rm reflectvlm
```

This will drop you into a bash shell inside the container. You can then run any of the commands below as described in the standard installation.

---

## Installation

1. Clone this repository
```bash
git clone git@github.com:yunhaif/reflect-vlm.git
cd reflect-vlm
```

2. Install packages
```bash
conda create -n reflectvlm python=3.9 -y
conda activate reflectvlm
pip install -e .
```

3. (Optional) Install additional packages if you want to train VLM policies.
```bash
pip install -e ".[train]"
pip install flash-attn --no-build-isolation
```

## Simulation environment
### Interacting with the environment
We provide a simple script to play with the simulation environment.
```bash
python scripts/interact.py
```
This will generate a task in MuJoCo with interactive visualization. You can interact with the environment by typing the actions. Just launch the script and follow the instructions â€” it works on Mac too!
```bash
mjpython scripts/interact.py    # Use `mjpython` to launch the script if you are on macOS
```
<center>
<img src="assets/images/demo_interact.gif" width=80%>
</center>

### Generating as many tasks as you want
The task generated by our procedural task generator is controlled by a seed. You can generate as many tasks as you want by simply changing the environment seed!
```bash
python scripts/interact.py --env_seed 1000001
```

## Policy evaluation

### Evaluating our checkpoints

Models are available on [Hugging Face](https://huggingface.co/collections/yunhaif/reflectvlm-67b95e4316ab2d5f71ad4b25), including:
* [`ReflectVLM-llava-v1.5-13b-base`](https://huggingface.co/yunhaif/ReflectVLM-llava-v1.5-13b-base): a base VLM policy trained on a fixed expert dataset.
* [`ReflectVLM-llava-v1.5-13b-post-trained`](https://huggingface.co/yunhaif/ReflectVLM-llava-v1.5-13b-post-trained): the VLM policy trained with our post-training strategy with reflection mechanism.
* [`ReflectVLM-diffusion`](https://huggingface.co/yunhaif/ReflectVLM-diffusion): the diffusion dynamics model.

We provide scripts to run evaluation on the 100 procedurally-generated test tasks. Models will be automatically downloaded from Hugging Face.

To evaluate the base policy:
```bash
bash scripts/eval_base_vlm.sh
```

To evaluate our post-trained policy with reflection:
```bash
bash scripts/eval_reflect_vlm.sh {sim|diffusion}
```
Choose either `sim` or `diffusion` as the dynamics model used in the reflection mechanism.

### Building your own agent
You can add your own agent under the `agent` folder. Create a new class and implement the `act()` method to process observation and get action.

```python
class MyAgent:
    def __init__(self, ...):
        ... # initialize model etc.

    def act(self, img, inp=None, next_image=None):
        """
        Args:
            img: the current image
            inp: the input prompt (optional)
            next_image: the next predicted image (optional, used in reflection)
        Returns:
            action: a string of action
        """
        action = ...    # get action from model
        return action
```

## Policy training
Coming soon...

## Diffusion model
The script `scripts/diffusion_demo.py` can be used to test diffusion generation:
```bash
python scripts/diffusion_demo.py
```
We provide some sample images under `assets/images/diffusion_examples`.

## Robot Pick Action Prediction

This directory contains scripts to predict robot scenes after pick actions using the ReflectVLM diffusion model.

### Available Scripts

#### 1. `robot_scene_analysis.py` (Recommended)
**Simple and comprehensive analysis of the robot scene**

```bash
python scripts/robot_scene_analysis.py
```

Features:
- Automatically analyzes the robot scene image (`robot_scene_20250731_164731.jpg`)
- Generates multiple pick action predictions with contextual prompts
- Saves results in `robot_predictions/` directory
- Includes original image for comparison

#### 2. `simple_robot_pick.py`
**Quick multiple predictions with predefined actions**

```bash
python scripts/simple_robot_pick.py
```

Features:
- Generates 6 different pick action predictions
- Saves results in `robot_pick_predictions/` directory

#### 3. `robot_pick_prediction.py`
**Advanced script with command-line options**

```bash
# Basic usage
python scripts/robot_pick_prediction.py --input robot_scene_20250731_164731.jpg --action "pick up the yellow object"

# Advanced usage with custom parameters
python scripts/robot_pick_prediction.py \
    --input robot_scene_20250731_164731.jpg \
    --action "grasp the red container" \
    --output my_prediction.png \
    --steps 50 \
    --img-guidance 1.5 \
    --text-guidance 10.0 \
    --seed 42
```

Options:
- `--input, -i`: Input robot scene image path
- `--action, -a`: Description of pick action
- `--output, -o`: Output path for generated image
- `--steps`: Number of inference steps (default: 50)
- `--img-guidance`: Image guidance scale (default: 1.5)
- `--text-guidance`: Text guidance scale (default: 10.0)
- `--seed`: Random seed for reproducibility

### Usage in Docker

```bash
# Start Docker container
docker compose run --rm reflectvlm

# Run any of the scripts
python scripts/robot_scene_analysis.py
```

### Example Output

The scripts will generate images showing what the robot scene might look like after performing the specified pick action. Results include:

- **Original scene**: The input robot scene
- **Pick predictions**: Generated images showing the scene after various pick actions
- **Contextual actions**: Actions tailored to visible objects in the scene

## Citation
If you find our work useful in your research, please consider citing with the following BibTeX:
```
@misc{feng2025reflective,
  title={Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation},
  author={Yunhai Feng and Jiaming Han and Zhuoran Yang and Xiangyu Yue and Sergey Levine and Jianlan Luo},
  year={2025},
  eprint={2502.16707},
  archivePrefix={arXiv},
  primaryClass={cs.RO},
  url={https://arxiv.org/abs/2502.16707},
}
```

## License & Acknowledgements

This repository is licensed under the MIT license. LLaVA is licensed under the [Apache 2.0 license](https://github.com/haotian-liu/LLaVA/blob/main/LICENSE).
Part of the simulation environment is adapted from [Metaworld](https://github.com/Farama-Foundation/Metaworld) and [mjctrl](https://github.com/kevinzakka/mjctrl).
